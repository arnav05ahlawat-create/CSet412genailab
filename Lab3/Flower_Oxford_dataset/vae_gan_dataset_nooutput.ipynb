{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "C2u8kYCx9LDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ee0JWxhj9LFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "2jfpjutZ9Pil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"/content/drive/MyDrive/flower_results\"\n",
        "\n",
        "vae_img_dir = OUTPUT_DIR + \"/vae_images\"\n",
        "vaegan_img_dir = OUTPUT_DIR + \"/vaegan_images\"\n",
        "model_dir = OUTPUT_DIR + \"/models\"\n",
        "plot_dir = OUTPUT_DIR + \"/plots\"\n",
        "\n",
        "for d in [vae_img_dir, vaegan_img_dir, model_dir, plot_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Folders created in Drive\")\n"
      ],
      "metadata": {
        "id": "5cOgODdt9Pgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = datasets.Flowers102(\n",
        "    root=\"/content/data\",\n",
        "    split=\"train\",\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "print(\"Total images:\", len(dataset))\n",
        "print(\"Batches per epoch:\", len(loader))\n"
      ],
      "metadata": {
        "id": "RCB53USt9Peq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, _ = next(iter(loader))\n",
        "grid = utils.make_grid(images[:16], normalize=True)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(grid.permute(1,2,0))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UhUMVve79PcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 128\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3,64,4,2,1), nn.ReLU(),\n",
        "            nn.Conv2d(64,128,4,2,1), nn.ReLU(),\n",
        "            nn.Conv2d(128,256,4,2,1), nn.ReLU(),\n",
        "            nn.Conv2d(256,512,4,2,1), nn.ReLU()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(512*4*4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(512*4*4, latent_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x).view(x.size(0), -1)\n",
        "        return self.fc_mu(x), self.fc_logvar(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 512*4*4)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512,256,4,2,1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256,128,4,2,1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,3,4,2,1), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self,z):\n",
        "        x = self.fc(z).view(-1,512,4,4)\n",
        "        return self.deconv(x)\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n"
      ],
      "metadata": {
        "id": "-0ZKMtOV9LN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def vae_loss(recon, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(recon, x)\n",
        "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss, kl_loss\n"
      ],
      "metadata": {
        "id": "bCTlTj3q9miu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "vae_recon_losses = []\n",
        "vae_kl_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    recon_epoch = 0\n",
        "    kl_epoch = 0\n",
        "\n",
        "    for imgs,_ in tqdm(loader):\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        mu, logvar = encoder(imgs)\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        z = mu + std*torch.randn_like(std)\n",
        "\n",
        "        recon = decoder(z)\n",
        "\n",
        "        recon_loss, kl_loss = vae_loss(recon, imgs, mu, logvar)\n",
        "        loss = recon_loss + kl_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        recon_epoch += recon_loss.item()\n",
        "        kl_epoch += kl_loss.item()\n",
        "\n",
        "    recon_epoch /= len(loader)\n",
        "    kl_epoch /= len(loader)\n",
        "\n",
        "    vae_recon_losses.append(recon_epoch)\n",
        "    vae_kl_losses.append(kl_epoch)\n",
        "\n",
        "    utils.save_image(recon[:16], f\"{vae_img_dir}/recon_epoch_{epoch}.png\", normalize=True)\n",
        "    utils.save_image(decoder(torch.randn(16,latent_dim).to(device)),\n",
        "                     f\"{vae_img_dir}/sample_epoch_{epoch}.png\", normalize=True)\n",
        "\n",
        "    torch.save({\"encoder\":encoder.state_dict(),\"decoder\":decoder.state_dict()},\n",
        "               f\"{model_dir}/vae_epoch_{epoch}.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: Recon={recon_epoch:.4f}, KL={kl_epoch:.4f}\")\n"
      ],
      "metadata": {
        "id": "vogxU7Yl9oHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({\n",
        "    \"Reconstruction Loss\": vae_recon_losses,\n",
        "    \"KL Loss\": vae_kl_losses\n",
        "}).to_csv(OUTPUT_DIR+\"/vae_losses.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "a9AP87R09pRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3,64,4,2,1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64,128,4,2,1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128,256,4,2,1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256,1,4,1,0), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x).view(-1)\n",
        "\n",
        "disc = Discriminator().to(device)\n"
      ],
      "metadata": {
        "id": "pW2g3qld9x1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "8ecB5Zu5GdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GAN_OUTPUT_DIR = \"/content/drive/MyDrive/flower_gan_results\"\n",
        "\n",
        "gan_img_dir = os.path.join(GAN_OUTPUT_DIR, \"images\")\n",
        "gan_model_dir = os.path.join(GAN_OUTPUT_DIR, \"models\")\n",
        "\n",
        "os.makedirs(gan_img_dir, exist_ok=True)\n",
        "os.makedirs(gan_model_dir, exist_ok=True)\n",
        "\n",
        "print(\"New GAN folders created:\")\n",
        "print(gan_img_dir)\n",
        "print(gan_model_dir)\n",
        "\n",
        "# verify\n",
        "os.listdir(GAN_OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "eS3AqclVHEz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision import utils\n",
        "\n",
        "# ===== NEW GAN OUTPUT FOLDER =====\n",
        "GAN_OUTPUT_DIR = \"/content/drive/MyDrive/flower_gan_results\"\n",
        "gan_img_dir = os.path.join(GAN_OUTPUT_DIR, \"images\")\n",
        "gan_model_dir = os.path.join(GAN_OUTPUT_DIR, \"models\")\n",
        "\n",
        "os.makedirs(gan_img_dir, exist_ok=True)\n",
        "os.makedirs(gan_model_dir, exist_ok=True)\n",
        "\n",
        "print(\"GAN folders ready:\", gan_img_dir, gan_model_dir)\n",
        "\n",
        "# ===== Hyperparameters =====\n",
        "epochs = 100\n",
        "\n",
        "lambda_recon = 10.0\n",
        "lambda_kl = 0.1\n",
        "lambda_adv = 1.0\n",
        "\n",
        "# ===== Optimizers =====\n",
        "opt_G = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
        "opt_D = optim.Adam(disc.parameters(), lr=5e-5)\n",
        "\n",
        "bce = nn.BCELoss()\n",
        "\n",
        "gan_losses = []\n",
        "recon_losses2 = []\n",
        "kl_losses2 = []\n",
        "\n",
        "# ===== Training Loop =====\n",
        "for epoch in range(epochs):\n",
        "    g_epoch = 0.0\n",
        "    d_epoch = 0.0\n",
        "    recon_epoch = 0.0\n",
        "    kl_epoch = 0.0\n",
        "\n",
        "    for imgs, _ in tqdm(loader):\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # ---- Encode & Decode ----\n",
        "        mu, logvar = encoder(imgs)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z = mu + std * torch.randn_like(std)\n",
        "        fake = decoder(z)\n",
        "\n",
        "        # ---- Train Discriminator ----\n",
        "        d_real = disc(imgs)\n",
        "        d_fake = disc(fake.detach())\n",
        "\n",
        "        # label smoothing\n",
        "        real_labels = 0.9 * torch.ones_like(d_real)\n",
        "        fake_labels = 0.1 * torch.zeros_like(d_fake)\n",
        "\n",
        "        d_loss = bce(d_real, real_labels) + bce(d_fake, fake_labels)\n",
        "\n",
        "        opt_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # ---- Train Generator (VAE-GAN) ----\n",
        "        d_fake = disc(fake)\n",
        "        adv_loss = bce(d_fake, torch.ones_like(d_fake))\n",
        "\n",
        "        recon_loss, kl_loss = vae_loss(fake, imgs, mu, logvar)\n",
        "\n",
        "        g_loss = (\n",
        "            lambda_recon * recon_loss +\n",
        "            lambda_kl * kl_loss +\n",
        "            lambda_adv * adv_loss\n",
        "        )\n",
        "\n",
        "        opt_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # ---- Accumulate losses ----\n",
        "        g_epoch += g_loss.item()\n",
        "        d_epoch += d_loss.item()\n",
        "        recon_epoch += recon_loss.item()\n",
        "        kl_epoch += kl_loss.item()\n",
        "\n",
        "    # ---- Average losses ----\n",
        "    g_epoch /= len(loader)\n",
        "    d_epoch /= len(loader)\n",
        "    recon_epoch /= len(loader)\n",
        "    kl_epoch /= len(loader)\n",
        "\n",
        "    gan_losses.append(g_epoch)\n",
        "    recon_losses2.append(recon_epoch)\n",
        "    kl_losses2.append(kl_epoch)\n",
        "\n",
        "    # ---- Save image safely ----\n",
        "    img_path = os.path.join(gan_img_dir, f\"epoch_{epoch+1}.png\")\n",
        "    utils.save_image(fake[:16], img_path, normalize=True)\n",
        "\n",
        "    # ---- Save model safely ----\n",
        "    model_path = os.path.join(gan_model_dir, f\"vaegan_epoch_{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        \"encoder\": encoder.state_dict(),\n",
        "        \"decoder\": decoder.state_dict(),\n",
        "        \"discriminator\": disc.state_dict()\n",
        "    }, model_path)\n",
        "\n",
        "    # ---- Print log ----\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "          f\"G: {g_epoch:.4f} | \"\n",
        "          f\"D: {d_epoch:.4f} | \"\n",
        "          f\"Recon: {recon_epoch:.4f} | \"\n",
        "          f\"KL: {kl_epoch:.4f}\")\n"
      ],
      "metadata": {
        "id": "5zyqkpo7CEOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gan = pd.DataFrame({\n",
        "    \"GAN Loss\": gan_losses,\n",
        "    \"Reconstruction Loss\": recon_losses2,\n",
        "    \"KL Loss\": kl_losses2\n",
        "})\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/flower_gan_results/gan_losses.csv\"\n",
        "df_gan.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"Losses saved to:\", csv_path)\n"
      ],
      "metadata": {
        "id": "T3Xy5IOpCEMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, len(gan_losses)+1)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs_range, gan_losses, label=\"GAN (Generator) Loss\")\n",
        "plt.plot(epochs_range, recon_losses2, label=\"Reconstruction Loss\")\n",
        "plt.plot(epochs_range, kl_losses2, label=\"KL Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"VAE-GAN Training Losses\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plot_path = \"/content/drive/MyDrive/flower_gan_results/loss_plot.png\"\n",
        "plt.savefig(plot_path)\n",
        "plt.show()\n",
        "\n",
        "print(\"Plot saved to:\", plot_path)\n"
      ],
      "metadata": {
        "id": "nUvpKJDvCEKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "vae_img_path = \"/content/drive/MyDrive/flower_results/vae_images/recon_epoch_10.png\"   # change epoch number\n",
        "gan_img_path = \"/content/drive/MyDrive/flower_gan_results/images/epoch_99.png\"        # change epoch number\n",
        "\n",
        "vae_img = Image.open(vae_img_path)\n",
        "gan_img = Image.open(gan_img_path)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(vae_img)\n",
        "plt.title(\"VAE Output (Blurry)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(gan_img)\n",
        "plt.title(\"VAE-GAN Output (Sharper)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LP6GzkK2JTj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhkwKz5GKlsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision import utils\n",
        "\n",
        "# ===== Paths =====\n",
        "GAN_OUTPUT_DIR = \"/content/drive/MyDrive/flower_gan_results\"\n",
        "gan_img_dir = os.path.join(GAN_OUTPUT_DIR, \"images\")\n",
        "gan_model_dir = os.path.join(GAN_OUTPUT_DIR, \"models\")\n",
        "\n",
        "os.makedirs(gan_img_dir, exist_ok=True)\n",
        "os.makedirs(gan_model_dir, exist_ok=True)\n",
        "\n",
        "# ===== Load last checkpoint =====\n",
        "last_epoch = 100   # change this to your last trained epoch number\n",
        "checkpoint_path = f\"{gan_model_dir}/vaegan_epoch_{last_epoch}.pt\"\n",
        "\n",
        "ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "encoder.load_state_dict(ckpt[\"encoder\"])\n",
        "decoder.load_state_dict(ckpt[\"decoder\"])\n",
        "disc.load_state_dict(ckpt[\"discriminator\"])\n",
        "\n",
        "print(\"Checkpoint loaded from epoch\", last_epoch)\n",
        "\n",
        "# ===== Hyperparameters =====\n",
        "more_epochs = 30\n",
        "lambda_recon = 10.0\n",
        "lambda_kl = 0.1\n",
        "lambda_adv = 1.0\n",
        "\n",
        "# ===== Optimizers =====\n",
        "opt_G = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)\n",
        "opt_D = optim.Adam(disc.parameters(), lr=5e-5)\n",
        "\n",
        "bce = nn.BCELoss()\n",
        "\n",
        "# ===== Resume Training =====\n",
        "for epoch in range(last_epoch, last_epoch + more_epochs):\n",
        "    g_epoch = 0.0\n",
        "    d_epoch = 0.0\n",
        "    recon_epoch = 0.0\n",
        "    kl_epoch = 0.0\n",
        "\n",
        "    for imgs, _ in tqdm(loader):\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # ---- Encode & Decode ----\n",
        "        mu, logvar = encoder(imgs)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        z = mu + std * torch.randn_like(std)\n",
        "        fake = decoder(z)\n",
        "\n",
        "        # ---- Train Discriminator ----\n",
        "        d_real = disc(imgs)\n",
        "        d_fake = disc(fake.detach())\n",
        "\n",
        "        real_labels = 0.9 * torch.ones_like(d_real)\n",
        "        fake_labels = 0.1 * torch.zeros_like(d_fake)\n",
        "\n",
        "        d_loss = bce(d_real, real_labels) + bce(d_fake, fake_labels)\n",
        "\n",
        "        opt_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # ---- Train Generator ----\n",
        "        d_fake = disc(fake)\n",
        "        adv_loss = bce(d_fake, torch.ones_like(d_fake))\n",
        "\n",
        "        recon_loss, kl_loss = vae_loss(fake, imgs, mu, logvar)\n",
        "\n",
        "        g_loss = (\n",
        "            lambda_recon * recon_loss +\n",
        "            lambda_kl * kl_loss +\n",
        "            lambda_adv * adv_loss\n",
        "        )\n",
        "\n",
        "        opt_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        g_epoch += g_loss.item()\n",
        "        d_epoch += d_loss.item()\n",
        "        recon_epoch += recon_loss.item()\n",
        "        kl_epoch += kl_loss.item()\n",
        "\n",
        "    # ---- Average losses ----\n",
        "    g_epoch /= len(loader)\n",
        "    d_epoch /= len(loader)\n",
        "    recon_epoch /= len(loader)\n",
        "    kl_epoch /= len(loader)\n",
        "\n",
        "    # ---- Save image ----\n",
        "    img_path = os.path.join(gan_img_dir, f\"epoch_{epoch+1}.png\")\n",
        "    utils.save_image(fake[:16], img_path, normalize=True)\n",
        "\n",
        "    # ---- Save checkpoint ----\n",
        "    model_path = os.path.join(gan_model_dir, f\"vaegan_epoch_{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        \"encoder\": encoder.state_dict(),\n",
        "        \"decoder\": decoder.state_dict(),\n",
        "        \"discriminator\": disc.state_dict()\n",
        "    }, model_path)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}] | \"\n",
        "          f\"G: {g_epoch:.4f} | \"\n",
        "          f\"D: {d_epoch:.4f} | \"\n",
        "          f\"Recon: {recon_epoch:.4f} | \"\n",
        "          f\"KL: {kl_epoch:.4f}\")\n"
      ],
      "metadata": {
        "id": "6HnuvmB8J2HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "vae_img_path = \"/content/drive/MyDrive/flower_results/vae_images/recon_epoch_10.png\"   # change epoch number\n",
        "gan_img_path = \"/content/drive/MyDrive/flower_gan_results/images/epoch_130.png\"        # change epoch number\n",
        "\n",
        "vae_img = Image.open(vae_img_path)\n",
        "gan_img = Image.open(gan_img_path)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(vae_img)\n",
        "plt.title(\"VAE Output (Blurry)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(gan_img)\n",
        "plt.title(\"VAE-GAN Output (Sharper)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4q_ha2DOLpFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDt0UN2cKrYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}