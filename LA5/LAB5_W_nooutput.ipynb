{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "k5l5wgypOmuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWzoMWPlGfEP"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CSET419 - Lab 5: Baseline CNN for Image-to-Image Translation\n",
        "# Production-Level Implementation with Google Drive Integration\n",
        "# ============================================================\n",
        "\n",
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 1: Mount Google Drive & Setup Environment          ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create project directory structure in Drive\n",
        "PROJECT_NAME = \"CSET419_Lab5_EncoderDecoder\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/{PROJECT_NAME}\"\n",
        "CHECKPOINT_DIR = f\"{BASE_PATH}/checkpoints\"\n",
        "RESULTS_DIR = f\"{BASE_PATH}/results\"\n",
        "LOGS_DIR = f\"{BASE_PATH}/logs\"\n",
        "CONFIG_PATH = f\"{BASE_PATH}/config.json\"\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [BASE_PATH, CHECKPOINT_DIR, RESULTS_DIR, LOGS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(f\"✓ Project mounted at: {BASE_PATH}\")\n",
        "print(f\"✓ Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"✓ Results: {RESULTS_DIR}\")\n",
        "print(f\"✓ Logs: {LOGS_DIR}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# Verify GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"✓ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✓ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Set PyTorch seeds\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True  # Enable for faster training\n"
      ],
      "metadata": {
        "id": "gxYDFRDOOnmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 3: Hyperparameter Configuration (PRO LEVEL)        ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "class HyperParams:\n",
        "    \"\"\"Professional hyperparameter configuration with validation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Data parameters\n",
        "        self.dataset = 'CIFAR10'\n",
        "        self.batch_size = 128\n",
        "        self.num_workers = 4\n",
        "        self.pin_memory = True\n",
        "\n",
        "        # Image parameters\n",
        "        self.image_size = 32\n",
        "        self.channels = 3\n",
        "        self.normalize_range = (-1, 1)  # Normalize to [-1, 1] as required\n",
        "\n",
        "        # Model architecture\n",
        "        self.encoder_blocks = [64, 128, 256, 512]  # Progressive feature extraction\n",
        "        self.bottleneck_dim = 512\n",
        "        self.use_batch_norm = True\n",
        "        self.use_dropout = True\n",
        "        self.dropout_rate = 0.2\n",
        "\n",
        "        # Training parameters\n",
        "        self.epochs = 100\n",
        "        self.learning_rate = 2e-4\n",
        "        self.weight_decay = 1e-5\n",
        "        self.scheduler_type = 'cosine'  # 'step', 'cosine', 'plateau'\n",
        "        self.warmup_epochs = 5\n",
        "\n",
        "        # Loss configuration\n",
        "        self.loss_type = 'combined'  # 'mse', 'l1', 'combined'\n",
        "        self.mse_weight = 0.5\n",
        "        self.l1_weight = 0.5\n",
        "        self.ssim_weight = 0.0  # Optional: add SSIM loss for better quality\n",
        "\n",
        "        # Optimization\n",
        "        self.optimizer = 'adamw'  # 'adam', 'adamw', 'sgd'\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.gradient_clip = 1.0\n",
        "\n",
        "        # Checkpointing\n",
        "        self.save_freq = 10  # Save every N epochs\n",
        "        self.keep_best = 3   # Keep top N best checkpoints\n",
        "\n",
        "        # Logging\n",
        "        self.log_freq = 50   # Log every N batches\n",
        "        self.sample_freq = 5  # Generate samples every N epochs\n",
        "\n",
        "        # Augmentation (for robustness)\n",
        "        self.use_augmentation = False  # Keep False for pure reconstruction\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {k: v for k, v in self.__dict__.items()}\n",
        "\n",
        "    def save(self, path: str):\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.to_dict(), f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str):\n",
        "        with open(path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "        hparams = cls()\n",
        "        for k, v in config.items():\n",
        "            setattr(hparams, k, v)\n",
        "        return hparams\n",
        "\n",
        "# Initialize hyperparameters\n",
        "hparams = HyperParams()\n",
        "hparams.save(CONFIG_PATH)\n",
        "print(\"✓ Hyperparameters configured and saved\")\n",
        "print(json.dumps(hparams.to_dict(), indent=2))"
      ],
      "metadata": {
        "id": "ZR1WAKceOy5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 4: Data Loading & Preprocessing (CIFAR10 Paired)   ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "class PairedCIFAR10Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Creates paired images for image-to-image translation.\n",
        "    For this lab, we use: Input = Slightly corrupted image, Target = Original\n",
        "    This creates a meaningful translation task while using CIFAR10.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, train: bool = True, transform=None, noise_factor: float = 0.1):\n",
        "        self.cifar10 = torchvision.datasets.CIFAR10(\n",
        "            root=root,\n",
        "            train=train,\n",
        "            download=True,\n",
        "            transform=None  # We'll handle transforms manually\n",
        "        )\n",
        "        self.transform = transform\n",
        "        self.noise_factor = noise_factor\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar10)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.cifar10[idx]\n",
        "\n",
        "        # Convert to tensor [0, 1]\n",
        "        if self.transform:\n",
        "            img_tensor = self.transform(img)\n",
        "        else:\n",
        "            img_tensor = transforms.ToTensor()(img)\n",
        "\n",
        "        # Create input by adding noise (denoising task)\n",
        "        # This is a valid image-to-image translation task\n",
        "        noise = torch.randn_like(img_tensor) * self.noise_factor\n",
        "        input_img = torch.clamp(img_tensor + noise, 0, 1)\n",
        "\n",
        "        # Normalize both to [-1, 1] as required\n",
        "        normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        input_img = normalize(input_img)\n",
        "        target_img = normalize(img_tensor)\n",
        "\n",
        "        return {\n",
        "            'input': input_img,\n",
        "            'target': target_img,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts [0,255] to [0,1]\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = PairedCIFAR10Dataset(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=train_transform,\n",
        "    noise_factor=0.1\n",
        ")\n",
        "\n",
        "test_dataset = PairedCIFAR10Dataset(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=test_transform,\n",
        "    noise_factor=0.1\n",
        ")\n",
        "\n",
        "# DataLoaders with optimized settings\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=hparams.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=hparams.num_workers,\n",
        "    pin_memory=hparams.pin_memory,\n",
        "    drop_last=True,\n",
        "    persistent_workers=True if hparams.num_workers > 0 else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=hparams.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=hparams.num_workers,\n",
        "    pin_memory=hparams.pin_memory,\n",
        "    persistent_workers=True if hparams.num_workers > 0 else False\n",
        ")\n",
        "\n",
        "print(f\"✓ Train samples: {len(train_dataset)}\")\n",
        "print(f\"✓ Test samples: {len(test_dataset)}\")\n",
        "print(f\"✓ Batches per epoch: {len(train_loader)}\")\n",
        "\n",
        "# Visualize sample\n",
        "sample = next(iter(train_loader))\n",
        "print(f\"✓ Input range: [{sample['input'].min():.2f}, {sample['input'].max():.2f}]\")\n",
        "print(f\"✓ Target range: [{sample['target'].min():.2f}, {sample['target'].max():.2f}]\")\n"
      ],
      "metadata": {
        "id": "4q4AHPUfO51u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 5: Encoder-Decoder Architecture (Professional)     ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Professional Conv Block with BN, Activation, and Dropout\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch: int, out_ch: int, downsample: bool = True,\n",
        "                 use_bn: bool = True, use_dropout: bool = False, dropout_rate: float = 0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # Convolution\n",
        "        if downsample:\n",
        "            layers.append(nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False))\n",
        "        else:\n",
        "            layers.append(nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False))\n",
        "\n",
        "        # Batch Normalization\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(out_ch))\n",
        "\n",
        "        # Activation\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True) if downsample else nn.ReLU(inplace=True))\n",
        "\n",
        "        # Dropout\n",
        "        if use_dropout and downsample:\n",
        "            layers.append(nn.Dropout2d(dropout_rate))\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "        # Skip connection if dimensions match\n",
        "        self.skip = (in_ch == out_ch) and not downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block for better gradient flow\"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels) if use_bn else nn.Identity()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels) if use_bn else nn.Identity()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return self.relu(out)\n",
        "\n",
        "class EncoderDecoderCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Professional Encoder-Decoder CNN for Image-to-Image Translation\n",
        "    Architecture: Input -> Encoder -> Bottleneck -> Decoder -> Output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hparams: HyperParams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        channels = hparams.channels\n",
        "        blocks = hparams.encoder_blocks\n",
        "\n",
        "        # ========== ENCODER ==========\n",
        "        self.encoder = nn.ModuleList()\n",
        "        in_ch = channels\n",
        "\n",
        "        for i, out_ch in enumerate(blocks):\n",
        "            self.encoder.append(\n",
        "                ConvBlock(\n",
        "                    in_ch, out_ch,\n",
        "                    downsample=True,\n",
        "                    use_bn=hparams.use_batch_norm,\n",
        "                    use_dropout=hparams.use_dropout and i < 2,  # Dropout only in early layers\n",
        "                    dropout_rate=hparams.dropout_rate\n",
        "                )\n",
        "            )\n",
        "            in_ch = out_ch\n",
        "\n",
        "        # Bottleneck with residual blocks for better representation\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            ResidualBlock(blocks[-1], hparams.use_batch_norm),\n",
        "            ResidualBlock(blocks[-1], hparams.use_batch_norm),\n",
        "            ResidualBlock(blocks[-1], hparams.use_batch_norm),\n",
        "        )\n",
        "\n",
        "        # ========== DECODER ==========\n",
        "        self.decoder = nn.ModuleList()\n",
        "        reversed_blocks = list(reversed(blocks))\n",
        "\n",
        "        for i in range(len(reversed_blocks) - 1):\n",
        "            in_ch = reversed_blocks[i]\n",
        "            out_ch = reversed_blocks[i + 1]\n",
        "\n",
        "            self.decoder.append(\n",
        "                ConvBlock(\n",
        "                    in_ch, out_ch,\n",
        "                    downsample=False,\n",
        "                    use_bn=hparams.use_batch_norm,\n",
        "                    use_dropout=False\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Final output layer (no batch norm, tanh activation for [-1, 1] range)\n",
        "        self.final = nn.Sequential(\n",
        "            nn.ConvTranspose2d(reversed_blocks[-1], channels, 4, stride=2, padding=1),\n",
        "            nn.Tanh()  # Output in [-1, 1]\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"He initialization for better convergence\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        skips = []\n",
        "        for enc in self.encoder:\n",
        "            x = enc(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder with skip connections (U-Net style)\n",
        "        for i, dec in enumerate(self.decoder):\n",
        "            x = dec(x)\n",
        "            # Add skip connection from encoder\n",
        "            if i < len(skips) - 1:\n",
        "                skip = skips[-(i+2)]\n",
        "                if x.shape == skip.shape:\n",
        "                    x = x + skip  # Residual connection\n",
        "\n",
        "        # Final output\n",
        "        x = self.final(x)\n",
        "        return x\n",
        "\n",
        "    def get_param_count(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "3-kKJd16PEgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = EncoderDecoderCNN(hparams).to(device)\n",
        "print(f\"✓ Model initialized\")\n",
        "print(f\"✓ Parameters: {model.get_param_count():,}\")\n",
        "print(model)\n",
        "\n",
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 6: Loss Functions & Metrics (Professional)         ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combined MSE + L1 Loss with optional SSIM\"\"\"\n",
        "\n",
        "    def __init__(self, hparams: HyperParams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.l1 = nn.L1Loss()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        loss = 0\n",
        "\n",
        "        if self.hparams.loss_type in ['mse', 'combined']:\n",
        "            loss += self.hparams.mse_weight * self.mse(pred, target)\n",
        "\n",
        "        if self.hparams.loss_type in ['l1', 'combined']:\n",
        "            loss += self.hparams.l1_weight * self.l1(pred, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class Metrics:\n",
        "    \"\"\"Track and compute metrics\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def psnr(pred, target, max_val=2.0):  # max_val=2 because range is [-1, 1]\n",
        "        \"\"\"Peak Signal-to-Noise Ratio\"\"\"\n",
        "        mse = torch.mean((pred - target) ** 2)\n",
        "        if mse == 0:\n",
        "            return float('inf')\n",
        "        return 20 * torch.log10(max_val / torch.sqrt(mse))\n",
        "\n",
        "    @staticmethod\n",
        "    def ssim(pred, target, window_size=11):\n",
        "        \"\"\"Structural Similarity Index (simplified)\"\"\"\n",
        "        # Simplified SSIM calculation\n",
        "        mu1 = torch.mean(pred)\n",
        "        mu2 = torch.mean(target)\n",
        "        sigma1 = torch.std(pred)\n",
        "        sigma2 = torch.std(target)\n",
        "        sigma12 = torch.mean((pred - mu1) * (target - mu2))\n",
        "\n",
        "        c1 = 0.01 ** 2\n",
        "        c2 = 0.03 ** 2\n",
        "\n",
        "        ssim_val = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / \\\n",
        "                   ((mu1**2 + mu2**2 + c1) * (sigma1**2 + sigma2**2 + c2))\n",
        "        return ssim_val\n",
        "\n",
        "# Initialize loss\n",
        "criterion = CombinedLoss(hparams).to(device)\n",
        "print(f\"✓ Loss function: {hparams.loss_type}\")\n",
        "print(f\"✓ MSE weight: {hparams.mse_weight}, L1 weight: {hparams.l1_weight}\")"
      ],
      "metadata": {
        "id": "yB60LK5VPLcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 7: Optimizer & Scheduler (Professional)            ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "def get_optimizer(model, hparams):\n",
        "    \"\"\"Configure optimizer based on hyperparameters\"\"\"\n",
        "\n",
        "    if hparams.optimizer == 'adam':\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=hparams.learning_rate,\n",
        "            betas=(hparams.beta1, hparams.beta2),\n",
        "            weight_decay=hparams.weight_decay\n",
        "        )\n",
        "    elif hparams.optimizer == 'adamw':\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=hparams.learning_rate,\n",
        "            betas=(hparams.beta1, hparams.beta2),\n",
        "            weight_decay=hparams.weight_decay\n",
        "        )\n",
        "    elif hparams.optimizer == 'sgd':\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=hparams.learning_rate,\n",
        "            momentum=0.9,\n",
        "            weight_decay=hparams.weight_decay\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer: {hparams.optimizer}\")\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def get_scheduler(optimizer, hparams):\n",
        "    \"\"\"Configure learning rate scheduler\"\"\"\n",
        "\n",
        "    if hparams.scheduler_type == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(\n",
        "            optimizer, step_size=30, gamma=0.5\n",
        "        )\n",
        "    elif hparams.scheduler_type == 'cosine':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "        )\n",
        "    elif hparams.scheduler_type == 'plateau':\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "    elif hparams.scheduler_type == 'onecycle':\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=hparams.learning_rate,\n",
        "            epochs=hparams.epochs,\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            pct_start=0.3\n",
        "        )\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "optimizer = get_optimizer(model, hparams)\n",
        "scheduler = get_scheduler(optimizer, hparams)\n",
        "\n",
        "print(f\"✓ Optimizer: {hparams.optimizer}\")\n",
        "print(f\"✓ Initial LR: {hparams.learning_rate}\")\n",
        "print(f\"✓ Scheduler: {hparams.scheduler_type}\")\n",
        "print(f\"✓ Weight decay: {hparams.weight_decay}\")\n"
      ],
      "metadata": {
        "id": "QcJEjRYZPS9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 8: Checkpoint Manager (Professional)               ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"Professional checkpoint management with best model tracking\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint_dir: str, keep_best: int = 3):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.keep_best = keep_best\n",
        "        self.best_losses = []  # List of (loss, path) tuples\n",
        "\n",
        "    def save(self, model, optimizer, scheduler, epoch: int,\n",
        "             train_loss: float, val_loss: float, is_best: bool = False):\n",
        "        \"\"\"Save checkpoint\"\"\"\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'hyperparameters': hparams.to_dict()\n",
        "        }\n",
        "\n",
        "        # Regular checkpoint\n",
        "        regular_path = f\"{self.checkpoint_dir}/checkpoint_epoch_{epoch:03d}.pt\"\n",
        "        torch.save(checkpoint, regular_path)\n",
        "        print(f\"✓ Saved checkpoint: {regular_path}\")\n",
        "\n",
        "        # Best checkpoint\n",
        "        if is_best:\n",
        "            best_path = f\"{self.checkpoint_dir}/best_model.pt\"\n",
        "            torch.save(checkpoint, best_path)\n",
        "            print(f\"✓ Saved best model: {best_path}\")\n",
        "\n",
        "            # Track best losses\n",
        "            self.best_losses.append((val_loss, best_path))\n",
        "            self.best_losses.sort(key=lambda x: x[0])\n",
        "\n",
        "            # Remove old best checkpoints if exceeding keep_best\n",
        "            if len(self.best_losses) > self.keep_best:\n",
        "                # Keep only the best ones\n",
        "                self.best_losses = self.best_losses[:self.keep_best]\n",
        "\n",
        "    def load_latest(self, model, optimizer, scheduler):\n",
        "        \"\"\"Load the most recent checkpoint\"\"\"\n",
        "        checkpoints = [f for f in os.listdir(self.checkpoint_dir)\n",
        "                      if f.startswith('checkpoint_epoch_')]\n",
        "\n",
        "        if not checkpoints:\n",
        "            return None, 0\n",
        "\n",
        "        # Sort by epoch number\n",
        "        checkpoints.sort()\n",
        "        latest = checkpoints[-1]\n",
        "        path = f\"{self.checkpoint_dir}/{latest}\"\n",
        "\n",
        "        return self.load(path, model, optimizer, scheduler)\n",
        "\n",
        "    def load_best(self, model, optimizer, scheduler):\n",
        "        \"\"\"Load the best checkpoint\"\"\"\n",
        "        path = f\"{self.checkpoint_dir}/best_model.pt\"\n",
        "        if os.path.exists(path):\n",
        "            return self.load(path, model, optimizer, scheduler)\n",
        "        return None, 0\n",
        "\n",
        "    def load(self, path: str, model, optimizer, scheduler):\n",
        "        \"\"\"Load specific checkpoint\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if scheduler and checkpoint['scheduler_state_dict']:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        epoch = checkpoint['epoch']\n",
        "        print(f\"✓ Loaded checkpoint from epoch {epoch}\")\n",
        "        return checkpoint, epoch\n",
        "\n",
        "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR, hparams.keep_best)\n",
        "print(f\"✓ Checkpoint manager initialized\")\n",
        "print(f\"✓ Keeping top {hparams.keep_best} best models\")\n"
      ],
      "metadata": {
        "id": "A9JIMiytPV7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 9: Training Loop (Professional with All Features)  ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Professional training manager\"\"\"\n",
        "\n",
        "    def __init__(self, model, criterion, optimizer, scheduler, hparams):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.hparams = hparams\n",
        "        self.writer = SummaryWriter(LOGS_DIR)\n",
        "        self.global_step = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.history = {\n",
        "            'train_loss': [], 'val_loss': [],\n",
        "            'train_psnr': [], 'val_psnr': [],\n",
        "            'learning_rates': []\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, dataloader, epoch):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_psnr = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{self.hparams.epochs} [Train]\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            inputs = batch['input'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if self.hparams.gradient_clip > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    self.model.parameters(),\n",
        "                    self.hparams.gradient_clip\n",
        "                )\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Metrics\n",
        "            with torch.no_grad():\n",
        "                psnr_val = Metrics.psnr(outputs, targets).item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_psnr += psnr_val\n",
        "            num_batches += 1\n",
        "\n",
        "            # Logging\n",
        "            if batch_idx % self.hparams.log_freq == 0:\n",
        "                self.writer.add_scalar('Loss/train_batch', loss.item(), self.global_step)\n",
        "                self.writer.add_scalar('PSNR/train_batch', psnr_val, self.global_step)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'psnr': f\"{psnr_val:.2f}\",\n",
        "                    'lr': f\"{self.optimizer.param_groups[0]['lr']:.6f}\"\n",
        "                })\n",
        "\n",
        "            self.global_step += 1\n",
        "\n",
        "            # Update scheduler if OneCycle\n",
        "            if isinstance(self.scheduler, optim.lr_scheduler.OneCycleLR):\n",
        "                self.scheduler.step()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_psnr = total_psnr / num_batches\n",
        "\n",
        "        return avg_loss, avg_psnr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, dataloader, epoch):\n",
        "        \"\"\"Validation loop\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_psnr = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        all_inputs = []\n",
        "        all_outputs = []\n",
        "        all_targets = []\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{self.hparams.epochs} [Val]\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            inputs = batch['input'].to(device)\n",
        "            targets = batch['target'].to(device)\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            psnr_val = Metrics.psnr(outputs, targets).item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_psnr += psnr_val\n",
        "            num_batches += 1\n",
        "\n",
        "            # Store samples for visualization\n",
        "            if len(all_inputs) < 64:  # Store up to 64 samples\n",
        "                all_inputs.append(inputs.cpu())\n",
        "                all_outputs.append(outputs.cpu())\n",
        "                all_targets.append(targets.cpu())\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'psnr': f\"{psnr_val:.2f}\"\n",
        "            })\n",
        "\n",
        "        # Concatenate samples\n",
        "        all_inputs = torch.cat(all_inputs)[:64]\n",
        "        all_outputs = torch.cat(all_outputs)[:64]\n",
        "        all_targets = torch.cat(all_targets)[:64]\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_psnr = total_psnr / num_batches\n",
        "\n",
        "        return avg_loss, avg_psnr, (all_inputs, all_outputs, all_targets)\n",
        "\n",
        "    def visualize_results(self, samples, epoch):\n",
        "        \"\"\"Save visualization of results\"\"\"\n",
        "        inputs, outputs, targets = samples\n",
        "\n",
        "        # Denormalize from [-1, 1] to [0, 1] for visualization\n",
        "        def denorm(x):\n",
        "            return (x + 1) / 2\n",
        "\n",
        "        inputs = denorm(inputs)\n",
        "        outputs = denorm(outputs)\n",
        "        targets = denorm(targets)\n",
        "\n",
        "        # Create grid: [Input | Output | Target]\n",
        "        comparison = torch.cat([inputs, outputs, targets], dim=0)\n",
        "        grid = make_grid(comparison, nrow=8, normalize=False, value_range=(0, 1))\n",
        "\n",
        "        # Save image\n",
        "        save_path = f\"{RESULTS_DIR}/epoch_{epoch:03d}.png\"\n",
        "        save_image(grid, save_path)\n",
        "\n",
        "        # Add to tensorboard\n",
        "        self.writer.add_image('Results/Input_Output_Target', grid, epoch)\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def train(self, train_loader, val_loader, checkpoint_manager):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Starting Training: {self.hparams.epochs} epochs\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        start_epoch = 1\n",
        "\n",
        "        # Try to resume from checkpoint\n",
        "        checkpoint, resumed_epoch = checkpoint_manager.load_latest(\n",
        "            self.model, self.optimizer, self.scheduler\n",
        "        )\n",
        "        if checkpoint:\n",
        "            start_epoch = resumed_epoch + 1\n",
        "            self.best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "            print(f\"Resuming from epoch {start_epoch}\")\n",
        "\n",
        "        for epoch in range(start_epoch, self.hparams.epochs + 1):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            # Train\n",
        "            train_loss, train_psnr = self.train_epoch(train_loader, epoch)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_psnr, samples = self.validate(val_loader, epoch)\n",
        "\n",
        "            # Scheduler step (if not OneCycle)\n",
        "            if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.OneCycleLR):\n",
        "                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Update history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['train_psnr'].append(train_psnr)\n",
        "            self.history['val_psnr'].append(val_psnr)\n",
        "            self.history['learning_rates'].append(current_lr)\n",
        "\n",
        "            # TensorBoard logging\n",
        "            self.writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "            self.writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "            self.writer.add_scalar('PSNR/train', train_psnr, epoch)\n",
        "            self.writer.add_scalar('PSNR/val', val_psnr, epoch)\n",
        "            self.writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "\n",
        "            # Visualization\n",
        "            if epoch % self.hparams.sample_freq == 0 or epoch == 1:\n",
        "                vis_path = self.visualize_results(samples, epoch)\n",
        "\n",
        "            # Checkpointing\n",
        "            is_best = val_loss < self.best_val_loss\n",
        "            if is_best:\n",
        "                self.best_val_loss = val_loss\n",
        "\n",
        "            if epoch % self.hparams.save_freq == 0 or is_best or epoch == self.hparams.epochs:\n",
        "                checkpoint_manager.save(\n",
        "                    self.model, self.optimizer, self.scheduler,\n",
        "                    epoch, train_loss, val_loss, is_best\n",
        "                )\n",
        "\n",
        "            # Print epoch summary\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            print(f\"\\nEpoch {epoch}/{self.hparams.epochs} Summary:\")\n",
        "            print(f\"  Time: {epoch_time:.2f}s | LR: {current_lr:.6f}\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f} | PSNR: {train_psnr:.2f}\")\n",
        "            print(f\"  Val Loss: {val_loss:.4f} | PSNR: {val_psnr:.2f}\")\n",
        "            print(f\"  Best Val Loss: {self.best_val_loss:.4f} {'⭐' if is_best else ''}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        # Save final history\n",
        "        history_path = f\"{RESULTS_DIR}/training_history.json\"\n",
        "        with open(history_path, 'w') as f:\n",
        "            json.dump(self.history, f, indent=4)\n",
        "\n",
        "        self.writer.close()\n",
        "        print(f\"\\n✓ Training completed! Best Val Loss: {self.best_val_loss:.4f}\")\n",
        "        print(f\"✓ Results saved to: {RESULTS_DIR}\")\n",
        "        print(f\"✓ Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
        "\n",
        "        return self.history\n"
      ],
      "metadata": {
        "id": "9DweCBmSPZcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 10: Initialize & Run Training                      ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(model, criterion, optimizer, scheduler, hparams)\n",
        "\n",
        "# Run training\n",
        "history = trainer.train(train_loader, test_loader, checkpoint_manager)"
      ],
      "metadata": {
        "id": "SpZWWYUDPlbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════════════════════════════════════════════╗\n",
        "# ║  CELL 11: Evaluation & Analysis                          ║\n",
        "# ╚══════════════════════════════════════════════════════════╝\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training curves\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
        "    axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
        "    axes[0, 0].set_title('Loss Curves')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # PSNR curves\n",
        "    axes[0, 1].plot(history['train_psnr'], label='Train PSNR')\n",
        "    axes[0, 1].plot(history['val_psnr'], label='Val PSNR')\n",
        "    axes[0, 1].set_title('PSNR Curves')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('PSNR (dB)')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Learning rate\n",
        "    axes[1, 0].plot(history['learning_rates'])\n",
        "    axes[1, 0].set_title('Learning Rate Schedule')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('LR')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # Loss difference (overfitting indicator)\n",
        "    diff = np.array(history['val_loss']) - np.array(history['train_loss'])\n",
        "    axes[1, 1].plot(diff, label='Val - Train Loss')\n",
        "    axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "    axes[1, 1].set_title('Generalization Gap')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss Difference')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/training_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Show final results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best Validation Loss: {min(history['val_loss']):.6f}\")\n",
        "print(f\"Best Validation PSNR: {max(history['val_psnr']):.2f} dB\")\n",
        "print(f\"Final Training Loss: {history['train_loss'][-1]:.6f}\")\n",
        "print(f\"Final Validation Loss: {history['val_loss'][-1]:.6f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display sample results\n",
        "from IPython.display import display\n",
        "latest_result = f\"{RESULTS_DIR}/epoch_{hparams.epochs:03d}.png\"\n",
        "if os.path.exists(latest_result):\n",
        "    img = Image.open(latest_result)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Results at Epoch {hparams.epochs} (Input | Output | Target)\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "etRiOwU2PnUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6N33DshbLGl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}