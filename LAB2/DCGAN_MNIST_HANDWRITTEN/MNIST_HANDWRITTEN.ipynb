{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vynyXC-EA9v"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# STEP 1: SETUP & IMPORTS\n",
        "# ===========================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# ===========================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ===========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "SyPktZyXEJ5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# PROJECT DIRECTORIES\n",
        "# ===========================\n",
        "PROJECT_NAME = \"MNIST_DCGAN\"\n",
        "BASE_DIR = f\"/content/drive/MyDrive/{PROJECT_NAME}\"\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
        "IMAGE_DIR = os.path.join(BASE_DIR, \"generated_images\")\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "PLOT_DIR = os.path.join(BASE_DIR, \"plots\")\n",
        "\n",
        "for folder in [BASE_DIR, CHECKPOINT_DIR, IMAGE_DIR, MODEL_DIR, PLOT_DIR]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(\"All folders created successfully\")\n"
      ],
      "metadata": {
        "id": "u9sUFbQDELyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# HYPERPARAMETERS\n",
        "# ===========================\n",
        "EPOCHS = 150          # More epochs = better quality\n",
        "BATCH_SIZE = 256\n",
        "LATENT_DIM = 100\n",
        "SAVE_INTERVAL = 10\n",
        "\n",
        "GEN_LR = 0.0002\n",
        "DISC_LR = 0.0002\n",
        "BETA_1 = 0.5\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "MamBD6Z8ELwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# LOAD MNIST DATASET\n",
        "# ===========================\n",
        "(train_images, _), (_, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape and normalize (-1 to 1)\n",
        "train_images = train_images.reshape(-1, 28, 28, 1).astype(\"float32\")\n",
        "train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "dataset = dataset.shuffle(60000).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(\"MNIST dataset loaded\")\n"
      ],
      "metadata": {
        "id": "sopq25OyELtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# GENERATOR MODEL\n",
        "# ===========================\n",
        "def build_generator():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(LATENT_DIM,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Reshape((7, 7, 256)),\n",
        "\n",
        "        layers.Conv2DTranspose(128, 5, strides=1, padding=\"same\", use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(64, 5, strides=2, padding=\"same\", use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(1, 5, strides=2, padding=\"same\",\n",
        "                               use_bias=False, activation=\"tanh\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()\n"
      ],
      "metadata": {
        "id": "aRUUqpScELqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# DISCRIMINATOR MODEL\n",
        "# ===========================\n",
        "def build_discriminator():\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(64, 5, strides=2, padding=\"same\",\n",
        "                      input_shape=[28, 28, 1]),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, 5, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "aDfUcx9vELoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# LOSS & OPTIMIZERS\n",
        "# ===========================\n",
        "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "generator_optimizer = keras.optimizers.Adam(GEN_LR, beta_1=BETA_1)\n",
        "discriminator_optimizer = keras.optimizers.Adam(DISC_LR, beta_1=BETA_1)\n"
      ],
      "metadata": {
        "id": "pzijt13qEpPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# TRAINING STEP\n",
        "# ===========================\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n"
      ],
      "metadata": {
        "id": "3uCHoBklI86D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# IMAGE SAVING FUNCTION\n",
        "# ===========================\n",
        "def save_generated_images(epoch, seed):\n",
        "    predictions = generator(seed, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 0.5 + 0.5, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    path = os.path.join(IMAGE_DIR, f\"epoch_{epoch:04d}.png\")\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "7dvuTX-JI_CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# TRAINING LOOP\n",
        "# ===========================\n",
        "seed = tf.random.normal([16, LATENT_DIM])\n",
        "history = {\"gen\": [], \"disc\": []}\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    gen_losses = []\n",
        "    disc_losses = []\n",
        "\n",
        "    for image_batch in dataset:\n",
        "        g_loss, d_loss = train_step(image_batch)\n",
        "        gen_losses.append(g_loss.numpy())\n",
        "        disc_losses.append(d_loss.numpy())\n",
        "\n",
        "    history[\"gen\"].append(np.mean(gen_losses))\n",
        "    history[\"disc\"].append(np.mean(disc_losses))\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | G: {history['gen'][-1]:.4f} | D: {history['disc'][-1]:.4f}\")\n",
        "\n",
        "    if epoch % SAVE_INTERVAL == 0:\n",
        "        save_generated_images(epoch, seed)\n",
        "\n",
        "        generator.save(os.path.join(CHECKPOINT_DIR, f\"generator_epoch_{epoch}.keras\"))\n",
        "        discriminator.save(os.path.join(CHECKPOINT_DIR, f\"discriminator_epoch_{epoch}.keras\"))\n"
      ],
      "metadata": {
        "id": "d7e88P4LJBq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# SAVE FINAL OUTPUTS\n",
        "# ===========================\n",
        "generator.save(os.path.join(MODEL_DIR, \"generator_final.keras\"))\n",
        "discriminator.save(os.path.join(MODEL_DIR, \"discriminator_final.keras\"))\n",
        "\n",
        "plt.plot(history[\"gen\"], label=\"Generator Loss\")\n",
        "plt.plot(history[\"disc\"], label=\"Discriminator Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(PLOT_DIR, \"loss_plot.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"Training Complete & Everything Saved to Drive\")\n"
      ],
      "metadata": {
        "id": "x8vsuDpHJBmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cc3Qmoo1JBkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aB0wU_mgJBGP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}